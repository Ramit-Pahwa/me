{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "\n",
    "## Username and Contact Information\n",
    "\n",
    "**Name**            :   Ramit Pahwa\n",
    "\n",
    "**University**      :   [Indian Institute of Technology (IIT), Kharagpur](http://iitkgp.ac.in)\n",
    "\n",
    "**Email**           :   ramitpahwa123@iitkgp.ac.in | ramitpahwa123@gmail.com\n",
    "\n",
    "**Github Username** :   [Ramit-Pahwa](https://github.com/Ramit-Pahwa)\n",
    "\n",
    "\n",
    "## Personal Background\n",
    "\n",
    "I am Ramit Pahwa, a fourth-year undergraduate pursuing Mathematics and Computing, IIT Kharagpur, India.\n",
    "\n",
    "My primary workstation has macOS High-Sierra and I use Atom as my preferred text editor.I have programming in C and C++ for the past 5 years and in python for the past 2 years or so and am proficient in all three of them . I am deeply passionate about Machine Learning and have been actively researching on Deep Learning Architectures.\n",
    "\n",
    "I am also familiar with git for version control and have successfully completed  projects/internship with large teams.\n",
    "\n",
    "My educational background is suffiecient for the project, as I have been exposed to various area's of mathematics through my curriculum which has imparted me a strong foundation.I am avid reader of fiction and true fan of FC Barcelona.\n",
    "\n",
    "\n",
    "## Previous Experience\n",
    "\n",
    "My first encounter with the Algebra was during my 3rd year of college when I took an Introduction course on Linear Algebra. As a part of the curriculum I have been constantly doing mathematical modelling in C++ and Python.(Links to repository below).\n",
    "I have expierence in working in a team through internships I did  at University of Alberta, where I was awarded with best student research paper, as well as working in developing production level Machine Learing/ Deep Learing solutions at Myntra.\n",
    "\n",
    "\n",
    "## Relevant Courses \n",
    "###Mathematics\n",
    "* Operation Research (Theory and Lab) [LINK](https://github.com/Ramit-Pahwa/Operations-Research)\n",
    "* Computational Linear Algerbra (ongoing)\n",
    "* Advanced Numerical Techniques (Theory and Lab ) [LINK](https://github.com/Ramit-Pahwa/Advance-Numerical-Techniques)\n",
    "* Linear Algebra \n",
    "\n",
    "###Computer Science \n",
    "* Algorithms And Data Structure (Theory and Lab)\n",
    "* Object Oriented Systems Design (Theory and Lab)\n",
    "* Computer Organization (Theory and Lab)\n",
    "* Machine Learning and Deep Learning (Theory and Project) \n",
    "\n",
    "\n",
    "## Answers of some questions\n",
    "\n",
    "`1. What do you want to have completed by the end of the program?`\n",
    "\n",
    "By the end of the program, I want to add support for solving generalised eigen-value problem efficiently and remove dependancy on ARPACK.\n",
    "\n",
    "`2. Who’s interested in the work, and how will it benefit them?`\n",
    "\n",
    "Many problems in applied sciences are posed as eigen-value problems an efficient sovler for large sparse matrix using Arnoldi method adds value to julia as package. \n",
    "\n",
    "`3. What are the potential hurdles you might encounter, and how can you resolve them?`\n",
    "\n",
    "Proposal for the project explain the above.\n",
    "\n",
    "`4. What other time commitments, such as summer courses, other jobs, planned vacations, etc., will you have over the summer?`\n",
    "\n",
    "I expect to work full time on the project that is 40 or more hours a week. I have no other commitments in the summer as of now.\n",
    "\n",
    "\n",
    "## Contribution to Open-Source Projects\n",
    "\n",
    "\n",
    "## Experience with Julia\n",
    "I have been using Julia for last one and half month. In terms of functionality, I like Julia because of its **multiple dispatch** feature as it lets me overload operators with a lot of ease than other programming languages.\n",
    "\n",
    "But the what intrigues me the most is ability to develop new packages without going into tht intricacies of low-level language and compromising with speed.Plus its similarity with python and Matlab make it easy to undersatand and reproduce.\n",
    "\n",
    "The most amazing feature of Julia is its parallelism, parallelize directly from command line by calling the desired script with a given number of cores.Additionally, it is also has the ability to assign task to different thread directly from the code.\n",
    "\n",
    "# The Project\n",
    "\n",
    "## The Problem and Motivations\n",
    "The objective of the project is to add iterative methods for for solving the (generalized) eigenvalue problem `Ax = λBx` .The goal of this project is to implement the Arnoldi or Lanczos method as a solver for the eigenvalue problem and to compare its performance with the ` eigs ` function.\n",
    "\n",
    "Many pratical problems in Computer Science and applied Mathematics are posed as eigen-value problems,Hence, an efficient and quick solver for the problem is highly desirable. A typical source of large scale eigenproblems is through a discrete form of a continuous problem. The resulting finite dimensional problems become large due to accuracy requirements and spatial dimensionality.To my undersatanding a correct approach would be to write a seperate package **IRAM.jl** dealing with generalized eigen-value problem through implentation of arnoldi method which is typically suitable for computation of small number of eigenvalues in cases where matrix is large and sparse.\n",
    "\n",
    "\n",
    "# The Plan\n",
    "I propose to implement required functionality in a package IRAM.jl tacking the problem in parts.First looking at the problem ` Ax = λx `, then extending the implementation to the generalised eigen value problem  `Ax = λBx` \n",
    "I plan to implement a 5 step procedure to implement the solver for generalised eigen value problem and socument my code as I progress as well as create a blog of the regular \n",
    "\n",
    "1. Add efficient Implementation of arnoldi factorization-base module.\n",
    "2. Add Implicitly restarted arnoldi method using efficient implementation of shifted QR strategy.\n",
    "3. Handle specific target eigenvalues (such as largest/smallest real/imaginary part or maximum absolute magnitude) along with various stopping criterion and deflation, \n",
    "4. Locking and Purging of ritz value through Orthogonal Deflating Transformation\n",
    "5. Implentation of solver for general problem `Ax = λBx` where ` B ` is symmetric positive definite (happens in FEM a lot) or potentially singular.\n",
    "**Implementation of Lancoz,block methods [post GSOC period]**\n",
    "\n",
    "I also aim to publish the work done during this period under the guidance of my mentorand continue to contribute and build the package beyond Summer of Code period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "First of all, we need to understand the mathematical formulation of Eigen problem before we get into the implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "**Hessenberg Matrix ** : In linear algebra, a Hessenberg matrix is a special kind of square matrix, one that is \"almost\" triangular. To be exact, an upper Hessenberg matrix has zero entries below the first subdiagonal, and a lower Hessenberg matrix has zero entries above the first superdiagonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "\\begin{bmatrix}1&4&2&3\\\\3&4&1&7\\\\0&2&3&4\\\\0&0&1&3\\\\\\end{bmatrix}\n",
    "is upper Hessenberg and\n",
    "\\begin{bmatrix}1&2&0&0\\\\5&2&3&0\\\\3&4&3&7\\\\5&6&1&1\\\\\\end{bmatrix}\n",
    "is lower Hessenberg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Krylov subspace :** In linear algebra, the order-r Krylov subspace generated by an n-by-n matrix A and a vector b of dimension n is the linear subspace spanned by the images of b under the first r powers of A (starting from  ` A^0=I A^0=I `), that is,\n",
    "\n",
    "$$ K_{r}(A, b) = span\\{b, Ab, A^2b, A^3b, ..., A^{r-1}b\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Orthonormal basis :** an orthonormal basis for an inner product space V with finite dimension is a basis for V whose vectors are orthonormal, that is, they are all unit vectors and orthogonal to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process we obtain an upper Hessemberg matrix, `H`, of dimension `m` whose eigenvalues are estimations of the dominant eigenvalues of matrix `A`. The process to build the matrix `H` is based on the following algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Arnoldi Algorithm\n",
    "\n",
    "1. ** START :** Choose an initial unitary vector, `x1`. This vector can be chosen by the user (random or determined) or from the restart file. Choose the Krylov subspace dimension, `m`. Choose the desired number of converged eigenvalues, ** nev **.\n",
    "2. Set $$ k = 1. w = Ax_{1}, h_{11} = x_1^Tw, f_1 = w - x_1h_{11} $$\n",
    "3. **Arnoldi Loop ** . Calculate\n",
    "   $$ for\\,\\, j = k, k+1, ... m-1 \\,\\,do $$\n",
    "   $$ h_{j+1,j}=\\lVert f_{j} \\rVert$$\n",
    "   $$ x_{j+1} = \\frac{f_{j}}{h_{j+1,j}} $$\n",
    "   $$ w = Ax_{j+1} $$\n",
    "   $$ h_{i,j+1} = x_{i}^Tw,\\, i=1,2,3 ...,j $$\n",
    "   $$ f_{j+1}=w-\\sum_{i=1}^jx_ih_{i,j+1} $$\n",
    "   $$ End\\,\\, do $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, the value of ` m ` is quite lower than the dimension of matrix  `A, n`, and if\n",
    "we only make one iteration of the algorithm, the eigenvalues of matrix `H` are not\n",
    "good approximations of the dominant eigenvalues of matrix `A` unless we choose the\n",
    "Krylov subspace dimension, `m`, very large. Thus, it is necessary to reinitialize the\n",
    "process. To do this, we compute the real Shur form of matrix `H`\n",
    "$$ T=Z_THZ $$\n",
    "and calculate\n",
    "$$ X=XZ$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of ` X`, normalized to one is taken as reinitialization vector, `x1` and\n",
    "the algorithm returns to step 2, if the process is not converged. If the `kth` eigenvalue\n",
    "is converged, we can make use of a deflacted process (Saad, 1992), set `k = k +1`, and\n",
    "return to step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `nev` eigenvalues of matrix `H` converge, we obtain as results of the process\n",
    "these eigenvalues as approximations for the `nev` dominant eigenvalues of matrix `A`,\n",
    "and the corresponding eigenvectors are linear combinations of the first `nev` columns\n",
    "of matrix `X`. Normally m is chosen higher than `2nev+1`. If `nev` is equal one, this\n",
    "means we are interested only on the fundamental mode, we can accelerate the process\n",
    "fixing the Krylov space dimension very high (9 or 10), or using Chebyshev\n",
    "polynomials.**below is implemented version of the arnoldi iteration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arnoldi_procedure (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for arnoldi method\n",
    "\n",
    "function arnoldi_procedure(A,m::Int64)\n",
    "    # Procedure to generate orthogonal basis of the Krylov subspace\n",
    "    n = size(A,1)\n",
    "    Q = Matrix{Float64}(n,m+1)\n",
    "    h = zeros(m+1,m)\n",
    "    v = rand(n)\n",
    "    w = Vector{Float64}(n)\n",
    "    b=view(Q,:,1)\n",
    "    copy!(b,v)\n",
    "    normalize!(b)\n",
    "    # fview(q,1)=normalize(V)\n",
    "    for j = 1:m\n",
    "        A_mul_B!(w,A,view(Q,:,j))\n",
    "\n",
    "        # Gram-Schmidt Orthogonalization\n",
    "        for i = 1:j\n",
    "            q1 = view(Q,:,i)\n",
    "            # h1 = view(h,i,j)\n",
    "            h[i,j] = dot(w,q1)\n",
    "            # zero dimensional subarray\n",
    "            w.-=q1.*h[i,j]\n",
    "        end\n",
    "        #h1 = view(h,j+1,j)\n",
    "        #copy!(h1,norm(w,1))\n",
    "        h[j+1,j] = norm(w,1)\n",
    "        if h[j+1,j] == 0 # Found an Invariant Subspace\n",
    "            return Q,h\n",
    "        end\n",
    "        q1=view(Q,:,j+1)\n",
    "        copy!(q1,w./h[j+1,j])\n",
    "    end\n",
    "    return Q,h\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The main computational cost of the algorithm is the product matrix-vector,\n",
    "because A is not given in explicit form. This process has a slow convergence and it\n",
    "needs a large number of iterations. An improvement of the basic algorithm, which\n",
    "reduces the number of matrix-vector products, is the Sorensen's IRA method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Implicitly Restarted Arnoldi Method (IRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRA method is scheduled in the following algorithm <br>\n",
    "** STEP 1 :** Perform the m-step Basic Arnoldi method to obtain `X, H and f_m` <br>\n",
    "** STEP 2 : ** Do until convergence \n",
    "1. Compute the eigenvalues of $H$, $\\theta_{1}$; ... $\\theta_{m}$, and their corresponding eigenvectors, $s_1$,.. $s_{m}$ and select p undesired eigenvalues $\\theta_{m-p+1}$,. . .,$\\theta_{m}$ ( $p$ must be such that $ nev < m-p$).Note that the eigenvalues $\\theta_{i}$ are approximations to the eigwnvalues of matrix A.\n",
    "2. Set $Q= (I)_{mxm}$ \n",
    "3. for j=1 ,2 ... p do\n",
    "    * Compute the QR Factorization \n",
    "    * $Q_{j}R\\,=\\,H-\\theta_{m-(p-1-j)}I$\n",
    "    * Compute $$H = Q^T_jHQ_j\\,\\,\\,Q=QQ_j$$\n",
    "4. End Do\n",
    "    * Set $X=XQ$\n",
    "    * Compute  $\\,\\,\\,f_k=x_{k+1}h_{k+1,k}+f_mq_{mk}$  where $k=m-p$\n",
    "    * Use the Arnoldi loop,of the the basic arnoldi algorithm ,with $k=m-p$ to obtain $X,H \\,\\,\\,and \\,\\,\\,f_m$\n",
    "    * Check the convergence criterion\n",
    "        $$ \\lVert f_m \\rVert | e_m^T | \\leq max(\\gamma_m \\lVert H \\rVert, tol |\\theta_i|)\\,\\, i=1,...,nev.$$\n",
    "        where $e_m^T=(0,0,0,..,1)_{1xm},\\,\\,\\,\\gamma_m$ is machine presicion and tol is prefixes tolerance.<br>\n",
    "End do \n",
    "\n",
    "<br>** STEP 3 : **When the matrix $X$ is converged, we store the first column, $x1$, in the restart file\n",
    "Once the process above has converged, to compute the dominant eigenvalues of $A$\n",
    "and the corresponding eigenvectors, the algorithm finishes with the following steps:\n",
    "1. Compute the partial Shur form\n",
    "    $$ HQ_{nev}=Q_{nev}R_{nev} \\,\\,\\, where\\,\\,\\, Q_{nev}\\epsilon>=R^{mxnev}\\,\\, R_{nev}\\epsilon>=R^{nevxnev}$$\n",
    "2. Store in the first $nev0$ columns of $X$:\n",
    "$$x_{i}=X(Q_{nev}),\\,\\,\\,\\, i=1,.., nev$$\n",
    "3. Compute the eigenvalues decomposition\n",
    "$$R_{nev}S_{nev}=S_{newnev}$$\n",
    "    where $S_{newnev}$ is diagonal matrixwhose elements are the obtained approximations to the dominant eigwnvalues of $A$\n",
    "4. The Corresponding eigenvectors are given by:\n",
    "$$X_{nev}=X_{nev}S_{nev}$$\n",
    "    where $X_{nev}$ are the matrix whose colunms are the vectors obtained above in 2\n",
    "\n",
    "\n",
    "This method takes advantage of the shifted QR iterations to accelerate the process.\n",
    "These iterations have a very low computational cost because the size of matrix\n",
    "$H$ is small and they reduce the number of large matrix-vector products, $Ax$, necessary\n",
    "for the convergence of the whole process.A good resource to understand subtlety in implenmtation is [1],[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91msyntax: use \"^\" instead of \"**\"\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91msyntax: use \"^\" instead of \"**\"\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "** pending algorithm implementation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Target eigen-value , stopping criterion and locking ,purging\n",
    "\n",
    "#### Defination\n",
    "\n",
    "** Deflation : **Deflation is an important concept in the practical implementation of the QR iteration and therefore equally important to the IRAM. In the context of the QR iteration, deflation amounts to setting a small subdiagonal element of the Hessenberg matrix $H$ to zero. This is called deflation because it splits the Hessenberg matrix into two smaller subproblems which may be independently refined further.\n",
    "\n",
    "In the Arnoldi procedure, as with a QR iteration, it is possible for some of the leading $k$ subdiagonals to become small during the course of implicit restarting. However, it is usually the case that there are converged Ritz values appearing in the spectrum of $H$ long before small subdiagonal elements appear. This convergence is usually detected through observation of a small last component in an eigenvector $y$ of $H$. When this happens, we are able to construct an orthogonal similarity transformation of $H$ that will give an equivalent Arnoldi factorization with a slightly perturbed $H$ that does indeed have a zero subdiagonal, and this is the basis of our deflation schemes.\n",
    "\n",
    "In the context of IRAM, there are two types of deflation required:<br>\n",
    "\n",
    "** Locking **:\n",
    "If a Ritz value $\\theta $ has converged (meaning $\\Vert Ax - x\\theta\\Vert < \\epsilon_D$) and thought to be a member of the wanted set of eigenvalues, then we wish to declare it converged, decouple the eigenpair $(x, \\theta)$, and continue to compute remaining eigenvalues with no further alteration of $x$ or $\\theta $. This process is called locking. [more info](http://www.netlib.org/utk/people/JackDongarra/etemplates/node229.html)<br>\n",
    "** Purging **:\n",
    "If a Ritz value $\\theta $ has converged but is not a member of the wanted set, then we wish to decouple and remove the eigenpair $(x, \\theta)$ from the current Krylov subspace spanned by the Arnoldi vectors. This process is called purging. \n",
    "[more info](http://www.netlib.org/utk/people/JackDongarra/etemplates/node230.html)\n",
    "\n",
    "\n",
    "### 4.Orthogonal Deflating Transformation\n",
    "\n",
    "A special orthogonal transformation to implement these deflation schemes. The deflation schemes are related to an eigenvector associated with a Ritz value that is to be deflated (either locked or purged). Given a vector $y$ of unit length, Algorithm computes an orthogonal matrix $Q$ such that $Q e_1 = y $ (hence $y = Q^* e_1 $).Algorithm is shown below \n",
    "\n",
    "![Image-1](http://www.netlib.org/utk/people/JackDongarra/etemplates/img2035.png)\n",
    "\n",
    "Further Details can be found at [info](http://www.netlib.org/utk/people/JackDongarra/etemplates/node227.html#lckform)\n",
    "\n",
    "### 5. Extending the Implementation for generalised eigen value problem  `Ax = λBx` \n",
    "\n",
    "** pending  **\n",
    "Information link [link](http://www.caam.rice.edu/software/ARPACK/UG/node33.html#shinv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Timeline (tentative)\n",
    "\n",
    "### Community Bonding period (22nd April - 14th May) \n",
    "\n",
    "My summer vacation will start from 30th of April. During this period, I would want to get myself more familiarized with the source code of ** IterativeSolvers.jl **. I would denote my time trying to solve issues,I believe solving the above issue would help me strengthen my understanding of source code and make myself comfortable with contributing to the package.\n",
    "\n",
    "### Week 1\n",
    "**Goal:** *Implement arnoldi factorization for $Ax=\\lambda x$ *\n",
    "\n",
    "I have already implemented a basic version of the procedure mentioned above .Will work towards implementing it in efficient and user friendly (with proper documentation) .Design of the function will mimic the interface for $eigs$ or Jacobi-Davidson packages.\n",
    "\n",
    "\n",
    "### Week 2\n",
    "**Goal:** *Implement Shifted QR-iteration. *\n",
    "\n",
    "In this week, I would add Implemntation of shifted QR-iteration.This algorithm will act as a base module for the impliciltly restarted arnoldi method.\n",
    "\n",
    "### Week 3 and 4\n",
    "**Goal:** *Implement the IRA Algorithm *\n",
    "\n",
    "During these weeks I will add IRA Algorithm, work on figuring out the ` qrfact()` algorithm, either using ARPACK routine or overloading the current QR factorization algorithm to cater to our need.\n",
    "\n",
    "\n",
    "\n",
    "### Week 5\n",
    "**Goal:** *Add support for different selection criteria *\n",
    "\n",
    "During this week, I would work to add functionality for user to target differnt types of eigen values for example : largest real part,smallest real part , largest imaginary part ,smallest imaginary part ,largest magnitude eigen values .\n",
    "\n",
    "### Week 6 and 7\n",
    "**Goal:** *Implement Orthogonal Deflating Transformation+Documentation*\n",
    "\n",
    "During these 2 weeks, I implement a new function which would to add Orthogonal Deflating Transformation functionality for IRAM.jl package, as well document and clean up my code base for the package .\n",
    "\n",
    "### Week 8\n",
    "**Goal:** *Add Locking and Purging functionality for Ritz Pair*\n",
    "\n",
    "During this week, I would be writing codes to use Locking and Purging for IRAM which ahs been verbally described below as :\n",
    "\n",
    "![Image-2](http://www.netlib.org/utk/people/JackDongarra/etemplates/img2077.png)\n",
    "\n",
    "\n",
    "\n",
    "### Week 9 and 10\n",
    "**Goal:** *Solver For `Ax = λBx`  ,Looking into Invert Spectral Transformation *\n",
    "\n",
    "During these two weeks, I plan to understand and implement Shift and Invert Spectral Transformation Mode to solve the general problem and\n",
    "preparing for a  release of the package.\n",
    "\n",
    "### Week 11 and beyod\n",
    "**Goal:** *Benchmarking Documentation*\n",
    "\n",
    "During these two weeks, I plan to understand clean up my code visualize and bechmark agaisnt other function to solve the eigenvalue problem.\n",
    "\n",
    "At the end of this week, the Iterative Solver in IRAM.jl would be ready for testing for the Julia community.\n",
    "\n",
    "### End-Term evaluation\n",
    "**Goal:** *Working towards the publication and final touches*\n",
    "\n",
    "Buffer period of 1 week for any lagging work is kept. I also aim to work towards writing a blog of my work and getting my work published.\n",
    "\n",
    "\n",
    "**Note 1**:I have no major plans for summer. I will try my best to learn as much as I can, during the project. My summer vacation starts on 30th April. Hence, I will start coding a 2 weeks earlier than the GSoC coding period, effectively giving me **14 weeks** to complete the project. Though my classes start in mid July, it won't be an issue as I won't have any tests or examination till the end of August."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "1. [ARPACK](http://www.caam.rice.edu/software/ARPACK/UG/node45.html#SECTION00800000000000000000)\n",
    "2. [Slides](http://people.bath.ac.uk/mamamf/talks/talk220405.pdf)\n",
    "3. [Book](http://people.inf.ethz.ch/arbenz/ewp/Lnotes/lsevp.pdf)\n",
    "4. [Book-2](http://www.netlib.org/utk/people/JackDongarra/etemplates/book.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
